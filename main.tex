\documentclass{article}
\usepackage{proof}
\usepackage{bussproofs}
\usepackage{xcolor}
\usepackage{url}
\usepackage{framed}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{mathtools}

\usepackage{hyperref}
\hypersetup{
 pdfborder={0 0 0},
 colorlinks=true,
 linkcolor=blue,
 urlcolor=blue,
 citecolor=blue
}
\usepackage{cleveref}


\newcommand{\rem}[1]{\textcolor{red}{[#1]}}
\newcommand{\ed}[1]{\textcolor{blue}{#1}}
\newcommand{\ct}[1]{\rem{#1 --ct}}

\begin{document}
\title{Automating Interactive Theorem Provers and Certifying Automatic Theorem Provers}
\author{Arjun Viswanathan}
\date{}
\maketitle
\begin{abstract}
	Interactive theorem provers (ITPs) and automatic 
	theorem provers (ATPs) are two distinct categories 
	of theorem provers on different ends of the spectrum 
	of theorem proving. On one hand, ITPs are typically 
	robust tools with a small, verified kernel, 
	making them highly reliable. However, they 
	require user intervention in the proving process, 
	only offering a minimal amount of automation. ATPs, 
	on the other hand, are push-button theorem provers 
	that use complex heuristics to prove theorems; as a consequence, they have a large code-base that is 
	hard to maintain and susceptible to bugs. A lot of 
	recent research has focused on bridging the gap 
	between these two poles of theorem proving. Hammers 
	and certified checkers are tools that were born from 
	this research, that have different approaches to 
	solving this problem. This work aims to 
	comprehensively study these different tools with a 
	focus on using SMT solvers as the ATPs enhancing 
	automation in ITPs.
\end{abstract}

\section{Introduction}
\label{sec:intro}
	Interactive theorem provers (ITPs) or proof 
	assistants are software tools that allow formalizing 
	of mathematical proofs. They provide an expressive 
	logic to state theorems in, and an interactive 
	interface through which the user can attempt to 
	prove these theorems using methods called tactics. 
	Generally, this interface mimics a written 
	mathematical proof with a context and a goal that 
	changes on the fly as one steps through the parts 
	of the proof. The data structures provided by the 
	ITP are minimal and the user's mathematical 
	structures are defined on top of these, keeping 
	the verified kernel of the ITP small. These proofs 
	provide a high level of reliability	but are hard to 
	come up with from the user's point of view. 
	
	Automatic theorem provers (ATPs) have grown rapidly 
	over the past decades and refer to tools that allow 
	automatic proving of logical formulas. Interaction 
	between the user and the ATP is kept to a minimum; 
	ideally, the user would provide a theorem to the ATP
	and the ATP either proves it or comes up with a counter-example that disproves it. Satisfiability 
	modulo theories (SMT) solvers and superposition 
	provers are two popular categories of ATPs.  
	Although both of these have different approaches to 
	proving, they both look at a formula in terms of 
	its satisfiability. The satisfiability problem is 
	the dual of the validity problem (proving a formula 
	to be valid) --- a formula can be proved to be valid 
	by establishing that its negation is unsatisfiable.
	
	\begin{figure}[t]
		\centering
		\includegraphics[scale=0.5]{coq.pdf}
		\caption{Comparing Interactive Theorem Provers 
		(ITPs) and Automatic Theorem Provers. Credits: 
		Chantal Keller (ATPs)}
		\label{fig:graph}
	\end{figure}

	ATPs and ITPs clearly have different modes of 
	operations and offer distinct advantages: ITPs 
	certify their results with a high degree of 
	trustworthiness although reaching the proofs might 
	take some time and effort from the user; in contrast, 
	ATPs offer relatively quick and automated results, 
	while not providing the same kind of guarantees that 
	ITPs do. A natural research question is whether we 
	can get the best of both worlds --- reliable proofs 
	with maximum automation. In this work, we study 
	various tools that attempt to combine ATPs and ITPs.
	
	After specifying the notation used in the rest of 
	the paper, we describe SMT solvers and superposition 
	provers in \Cref{sec:atp}.
	\Cref{sec:itp} explores the state-of-the-art 
	of ITPs. \Cref{sec:hammer} and 
	\Cref{sec:sledgehammer}	explore hammers in general, 
	and Sledgehammer's integration with SMT solvers in
	particular. In \Cref{sec:cert}, we look at SMTCoq. 
	Finally, we compare the 2 approaches in 
	\Cref{sec:conc}, then present some related work, 
	and discuss our plans to take this research forward.

\section{Technical Preliminaries}
\label{sec:prelim}
	Most of the notation and technicalities are 
	introduced where needed in this document. For a 
	thorough introduction to first order logic and 
	satisfiability modulo theories, 
	see~\cite{DBLP:reference/mc/BarrettT18}.
	Briefly, literals are variables or negations of 
	variables, terms are built from literals, and 
	formulas are built from terms and quantifiers. 
	Most provers reason over arbitrary logical 
	formulas which they convert into a normal form 
	for easier processing. One such normal form is 
	the conjunction nomal form (CNF). A formula in 
	CNF is a conjunction of clauses (disjunctions 
	of literals). Additionally, SMT deals with a 
	sorted FOL - sort is a formal synonym for type. 
	
	We also look at various rule systems. In 
	general a rule looks like this:
	\begin{center}
		$\infer[rule-name]{conclusion}
			{premise_1 & premise_2 & ... & premise_n}$
	\end{center}
	The rule called \textit{rule-name} has $n$ 
	premises $premise_1, ... , premise_n$ and a 
	conclusion $conclusion$. The rule can be 
	understood as follows --- if all $n$ premises 
	are true, then it follows that the $conclusion$
	is true. Usually, the premises and the conclusion 
	are formulas but the Z3 SMT solver uses a 
	sequent calculus where these are sequents. 
	These are introduced later before their usage.
	
	We discuss various proof systems that give 
	refutation proofs of validity. A refutation 
	proof employs the duality between validity 
	and satisfiability. To prove a formula 
	$F$, it suffices to prove that $\neg F$ is 
	unsatisfiable. If there is no way to satisfy 
	$\neg F$, then by refutation, $F$ must be true.
	This form of proofs is a recurring theme in 
	this work.
	
	The following abbreviations are repeatedly used:
	SAT (SAT), satisfiability modulo theories (SMT),
	unsat (unsatisfiable), first-order logic (FOL), 
	higher-order logic (HOL).
	
	
\section{Automatic Theorem Provers}
\label{sec:atp}	
	\subsection{SMT Solvers}
	\label{smt}
		Boolean satisfiability, often called the SAT 
		problem, is the problem of determining if a 
		Boolean formula	is satisfiable, that is, 
		whether there is an assigment of the values 
		of $True$ or $False$ to the variables of the 
		formula so that the entire formula evaluates 
		to $True$. For example,
			\begin{center}
				$(x \lor y) \land z$ 
			\end{center}
		can be satisfied by the 
		assignment $\{x=True,\ y=False,\ z=True\}$. On 
		the other hand, 
			\begin{center} 
				$x \land \neg x$ 
			\end{center}
		is unsatisfiable, no matter what value is 
		assigned to $x$.
	
		Satisfiability Modulo Theories~\cite{DBLP:reference/mc/BarrettT18} 
		or SMT lifts SAT to a level that includes 
		theories, by considering formulas whose atoms 
		can be not just propositional variables but 
		also atomic formulas in some logical theory 
		of interest.
		For example, 
			\begin{center} 
				$(a = b) \land (b = c) \land \neg (a = c)$ 
			\end{center}
		is a formula that is unsatisfiable in the 
		theory of equality over uninterpreted functions, 
		in which we have the axioms of equality and the 
		only information about functions/constants we 
		have is syntactical. 
		This formula is ``unsat" because, by	
		transitivity of $a = b$ and $b = c$, we have 
		$a = c$. SMT allows us to be more expressive with 
		our formulas, but this comes at the cost of more complicated procedures.
	
		SMT solvers have plenty of applications in 
		formal methods and software verification. 
		For instance, SMT solvers are used in the 
		back-end of model
		checkers~\cite{DBLP:books/daglib/0020348}, 
		which take as input mathematical models of a 
		software system, and verify whether they satisfy 
		a particular temporal property or not. Another 
		area of application is symbolic
		execution~\cite{DBLP:journals/csur/BaldoniCDDF18}, 
		which is to analyze multiple execution paths 
		of a program based on abstractions of the 
		inputs. Other uses of SMT solvers include 
		program synthesis~\cite{synth}, static analysis, 
		and generation of logical
		interpolants~\cite{DBLP:journals/corr/abs-1111-5652}.
	
		Due to the recent emphasis on verifying results 
		from SMT solvers~\cite{10.1145/1670412.1670413,
		mansur2020detecting, 10.1007/978-3-642-38916-0_3},
		many SMT solvers are proof producing. When an 
		SMT solver finds that a (quantifier-free) formula 
		is satisfiable, an easily checkable 
		\emph{certificate} of this is a satisfying model 
		of the formula.	However, when a solver 
		concludes that a formula is unsatisfiable, it is 
		more difficult to produce an acceptable certificate 
		since this certificate must contain, in effect, 
		a proof of unsatisfiability. Most SMT solvers 
		follow some version of the DPLL(T) algorithm, 
		which tries	to satisfy the formula by propagating assignments, making choices on assignments when necessary, and concluding ``unsat'' when all choices 
		have been tried unsuccessfully. Since the resolution 
		rule is central to this algorithm, a universally 
		accepted proof calculus is one based on resolution. Specifically, a proof of unsatisfiability of a 
		formula in conjunction normal form (CNF), is a 
		tree with the input clauses and theory lemmas at 
		the leaves, and the empty clause at the root. Each 
		node of the tree is an application of rules that simplifies previous nodes. The primary rule used in 
		these trees is resolution.
	
		The resolution rule takes two clauses, a pivot 
		element that occurs with opposite polarities in 
		each of the clauses, and gives one clause that is 
		a consequent of the premises. The idea is that, 
		beginning with the input clauses and the theory 
		lemmas (which might have their own sub-proof trees 
		coming from separate internal solvers called 
		theory solvers), the tree derives that the empty 
		clause holds, which is the most basic notion of unsatisfiability since the there is no way to 
		satisfy the empty clause.
		\begin{center}
			$\infer[resolution]{\varphi_1 \lor ... \lor \varphi_n \lor 
			\psi_1 \lor ... \lor \psi_m}
			{\varphi_1 \lor ... \lor \varphi_n \lor \chi & \neg \chi 
			\lor \psi_1 \lor ... \lor \psi_m}$ 
		\end{center}
		For instance, the following is a valid application 
		of the resolution rule.
		\begin{center}
			$\infer{a \lor c}{a \lor \neg b & b \lor c}$
		\end{center}
	
		\begin{figure}[t]
			\fbox{\includegraphics[scale=0.5]{prooftree.pdf}}
			\caption{A proof tree of unsatisfiability.}
			\label{fig:tree}
		\end{figure}

		The proof calculus for an SMT solver consists 
		of proof rules such as resolution and a 
		proof of unsatisfiability is a proof tree with 
		the assertions asserted in the problem as 
		leaves, as illustrated in \Cref{fig:tree}.
		Additionally, clauses asserted to 
		be true by a theory solver --- called theory lemmas 
		- can also be leaves. These are usually 
		justified by the theory solver, possibly by 
		a tree rooted at the lemma. Each node in 
		the tree is an application of a proof rule, 
		and the root concludes the empty clause $\bot$.
	
		Even though this sort of calculus is common among 
		proof-producing SMT solvers, there is no standard 
		one and each solver uses its own variation.
		For example, CVC4~\cite{DBLP:conf/cav/BarrettCDHJKRT11} 
		uses a general proof framework called LFSC or 
		Logical Framework with Side Conditions
		that mixes declarative rules with computable programs; 
		veriT~\cite{10.1007/978-3-642-02959-2_12} uses a 
		rule-based calculus defined in a 
		SMT-LIB-like syntax; 
		Z3~\cite{10.1007/978-3-540-78800-3_24} uses a 
		rule-based calculus with a relatively lower level 
		of granularity.
	
	\subsection{Superposition Provers}
	\label{sup}
		Superposition provers or resolution provers differ 
		from SMT solvers in that they focus on proving 
		conjectures rather than finding a satisfying model
		for a set of formulas. While these problems are 
		duals of each other, picking one over the other 
		does make a difference in the kinds of problems 
		that become solvable due to the complexity of 
		the problem space - the SAT problem is NP-complete
		and quantification and theory reasoning often lead 
		to undecidability. The input problem to a 
		superposition prover is formulated as a set of 
		axioms that relate to the problem space, 
		a set assumptions and a conjecture to prove. 
		Additionally, while theories are built-in to SMT 
		solvers, they need to be externally axiomatized 
		for resolution provers. As such, superposition 
		provers are better suited for quantified formulas 
		and minimal theory reasoning, whereas SMT solvers 
		do well on problems that contain constraints 
		in theories and quantified formulas slow them 
		down. Because theories aren't built-in to 
		superposition provers, they distinguish axioms 
		and assumptions in their inputs. Superposition 
		provers, like SMT-solvers, produce resolution 
		proofs.

		The goal is to find the unsatisfiability 
		of the negation of the conjecture along with the 
		assumptions and the axioms, and this is done 
		by refutation. The prover converts 
		the input formulas into a set of clauses and 
		uses a small set of inference rules to add 
		implied clauses to the input set of 
		clauses --- ultimately it tries to derive 
		the empty clause or figure out that this 
		is not possible. These provers are 
		resolution-based --- the most important rule 
		in the calculus is the 
		\textit{resolution} rule. This has evolved 
		into the \textit{paramodulation} rule to 
		accommodate the notion of equality, and 
		ultimately into the \textit{superposition} rules 
		that take into consideration, along with 
		equality, an ordering on the terms in the clauses 
		to reduce the number of rule applications.

		Conceptually, a proof trace is a tree with the 
		empty clause at the root, and the 
		axioms/assumptions at the leaves. Each node is 
		obtained using one or more inference rules applied 
		to the previous nodes/leaves. In practice, the 
		tree is implemented as a directed acyclic graph 
		(DAG) since nodes are shared. The proof calculus 
		is sound and refutationally complete --- if the 
		input is unsatisfiable, the empty clause will be 
		derived. Termination, however, depends on the
		availability of resources (time and memory) when 
		the input is satisfiable, and thus isn't 
		guaranteed. 

		Popular resolution provers such as
		Vampire~\cite{10.1007/978-3-642-39799-8_1}, 
		E~\cite{10.5555/1218615.1218621}, and 
		SPASS~\cite{10.1007/978-3-642-02959-2_10} adhere 
		to the TPTP (Thousands of Problems 
		for Theorem 
		Provers)~\cite{Sut17} input/output standard.

		A superposition prover applies the inference 
		rules to the input set of 
		clauses until saturation --- that is until the 
		resultant set of clauses is closed under the 
		inference rules. To keep this system
		efficient, provers use an ordering 
		on the terms. The saturation algorithm used 
		by the prover guides the process of choosing 
		the next inference rule to apply based on this 
		term ordering. 

		Given $S$, the set of input clauses, the possible 
		outcomes of a superposition prover are:
		\begin{enumerate}
		\item The empty clause is derived from $S$, and
			$S$ is unsatisfiable.
		\item The solver terminates without generating the 
			empty clause, and $S$ is satisfiable.
		\item The saturation algorithm does not terminate 
			before the prover runs out of resources, and 
			the empty clause is not generated. In this 
			case, the result is unknown. 
		\end{enumerate}

		Since there may be redundancy in the application 
		of inference rules and the generated clauses, 
		many provers work on minimizing this redundancy 
		in the interest of efficiency.

		Resolution provers deal with theories by 
		adding a user-provided axiomatization of the 
		theory to the leaves of the proof DAG before 
		running the saturation algorithm.
	
\section{Proof Assistants}
\label{sec:itp}
A proof assistant or interactive theorem prover 
(ITP) is a software tool used to formalize
proofs by human-machine collaboration.
Proof assistants originated with the 
Automath~\cite{10.1007/BFb0060623} and 
the Logic for Computable Functions (LCF) 
theorem prover~\cite{10.5555/891954}, 
which is designed in the logic with the 
same name~\cite{Loeckx1987}. Both emphasized the 
principle of having a small trustable proof 
checker. In LCF-style theorem provers, theorems 
are implemented as an abstract datatype, and 
new theorems can only be constructed through a 
fixed set of functions, that correspond to the 
underlying logic's axiom schemata and inference 
rules, provided by this datatype. This keeps
the trusted codebase to a minimum, and 
provides strong soundness guarantees. LCF-style 
theorem provers
include the HOL family of ITPs: 
HOL Light~\cite{10.1007/978-3-642-03359-9_4}, 
HOL4~\cite{10.1007/978-3-540-71067-7_6} and
Isabelle~\cite{10.1007/978-3-540-71067-7_7}. 
Automath is based on type theory and the 
Curry-Howard isomorphism, where 
propositions or theorem statements are types, 
and their proofs are programs inhabiting them. 
ITPs taking the Automath approach to 
theorem-proving include Coq~\cite{Thery2006},
Agda~\cite{10.1007/978-3-642-03359-9_6}, 
NuPRL~\cite{Jac96}, 
Matita~\cite{10.5555/2032266.2032273}, 
and Lean~\cite{10.1007/978-3-319-21401-6_26}.

In the following sections, we focus on 
particular ATP integrations with Isabelle/HOL 
and Coq. Isabelle is a generic proof assistant 
that can be instantiated with a particular 
logic and calculus. Isabelle/HOL is an instance 
of Isabelle that provides an expressive HOL to 
prove theorems in its proof language Isar, 
and also consists of a 
large library of formally verified mathematics. 
The Coq proof assistant provides a functional 
programming language interface in Gallina, and 
implements a type theory called the calculus of 
inductive constructions (CIC). Similar to 
Automath, theorems are types in Coq, and 
proofs are programs inhabiting them. While these 
proofs are written manually by the user, Coq 
provides some automation in the form of 
\textit{tactics}. Using tactics, the user 
must build a term whose type is the theorem 
being proved, and a proof ends with 
\texttt{Qed} which calls the Coq type checker 
to certify the proof.	
	
\section{Hammers}
\label{sec:hammer}
	Hammers are automated tools that use ATPs to 
	help automate the proving of goals within ITPs
	that depend on certain other lemmas. A hammer 
	provides a method for automation within the 
	ITP as follows. When a user calls the hammer's 
	tactic on a goal, it sends the goal 
	along with a set of premises (previously proven 
	lemmas) to the ATP, and if the ATP is able to 
	prove the goal, it processes this proof in some 
	way within the ITP to give the user a closed 
	proof of the goal. A hammer is composed of 
	a premise selector, which is a method to identify 
	relevant premises that help prove a goal; a 
	translator that bridges the gap between the logic 
	of the ITP and that of the ATP(s); 
	and a proof reconstructor that reconstructs in the 
	ITP the external proof provided by the ATPs.
	
	\subsection{Premise Selection}
		A typical ITP goal $L$ has a set of hypotheses 
		$H_1, H_2, ..., H_n$ and a conjecture $G$ to 
		prove:
		\begin{center}
			$L : H_1 \to H_2 \to ... \to H_n \to G$.
		\end{center}
		Proving $L$ is equivalent to proving the unsatisfiability 
		of $\neg L$.
		\begin{align*}
			\neg L &= \neg (H_1 \to H_2 \to ... \to H_n \to G)\\
			&= \neg (\neg H_1 \lor \neg H_2 \lor ... \lor \neg H_n \lor G)
			& \text{by unfolding }\to \\
			&= \neg \neg H_1 \land \neg \neg H_2 \land ... \land \neg \neg H_n 
			\land \neg G
			& \text{by De Morgan's law}\\
			&= H_1 \land H_2 \land ... \land H_n \land \neg G
			& \text{by double negation elimination}
		\end{align*}
	
		So the problem can be equivalently stated in the context of ATPs
		as checking the unsatisfiability of a set of hypotheses 
		along with the negation of the conjecture. For instance, in an SMT-LIB file, this would like this:
		\begin{verbatim}
			assert H_1
			assert H_2
			...
			assert H_n
			assert (not G)
			check-sat
		\end{verbatim}
		However, goals in ITPs aren't necessarily 
		self-contained as in $L$. The hypotheses needed to 
		prove $G$ might have been proven earlier. In fact, 
		ITPs have large	libraries of proven lemmas, any of 
		which might be useful in proving a goal $G$. As 
		such, picking the right hypotheses to send 
		along with the negation of the goal to an ATP is challenging. This process is called premise 
		selection. The most basic form of premise selection 
		simply leaves it to the user to find relevant facts 
		to prove a conjecture. But given that these libraries 
		have hundreds of lemmas, using automatic methods 
		to select premises is a more scalable and 
		generalizable approach.
	
		Modern hammers have plenty of ways to 
		automate premise selection. Some of them use 
		machine learning to learn axiom selection from 
		previously successful proofs using 
		a variety of methods based on Bayesian 
		statistics~\cite{DBLP:journals/jar/AlamaHKTU14}, 
		nearest-neighbor ranking 
		functions~\cite{DBLP:conf/cade/KaliszykU13a}, and
		non-linear kernel
		methods~\cite{DBLP:journals/jar/AlamaHKTU14}.
		Others use simpler algorithms; the relevance filter 
		by Meng and Paulson~\cite{DBLP:journals/japll/MengP09}
		for the Isabelle/HOL ITP selects relevant facts by 
		giving priority to those with a larger number of 
		symbols in common with the goal. The Divvy 
		system~\cite{10.1007/978-3-642-02959-2_13} also 
		uses a syntactic relevance filtering technique; 
		it also uses an ordering technique based 
		on latent semantics called APRILS (Automated 
		Prophesier of Relevance Incorporating Latent 
		Semantics). Others use semantics instead of 
		syntax to guide	the selection process. For 
		instance, the SRASS (Semantic Relevance Axiom 
		Selection 
		System)~\cite{10.1007/978-3-540-73595-3_20} 
		finds countermodels of the conjecture and selects 
		axioms that	exclude the countermodels. Other work combines these various types of methods to increase efficiency~\cite{DBLP:journals/corr/KaliszykU13b, 
		10.1007/978-3-642-31365-3_30}.
		
		\subsection{Translation}
		ITPs use expressive higher-order logics
		sometimes with set-theoretic notation, whereas 
		ATPs are mostly restricted to first-order logic 
		(FOL). Thus, not all ITP problems are transferable 
		to an ATP. However, there is a large enough subset 
		of problems provable by an ATP that can help an 
		ITP user. When a goal is provable by an ATP, it 
		needs to be soundly translated from the ITP's 
		logic to that of the ATP. The translation can 
		involve various tricks to eliminate higher-order 
		constructs~\cite{DBLP:journals/jar/MengP08} 
		such as higher-order quantification, partial 
		function applications, and anonymous functions
		from the goal; and an encoding of complex 
		types into simpler types, where applicable.
		Furthermore, resolution-based 
		provers usually do not have built-in theories, so a 
		theory must be externally axiomatized within the 
		ATP. With SMT solvers, there maybe a stronger
		correspondence between some ITP types and 
		sorts in the SMT solver, so the respective 
		theory might serve as an axiomatization.

		\subsection{Proof Reconstruction}
		Once the goal is translated to a problem understandable 
		to the ATP, the result from the ATP needs to be 
		soundly translated back to a valid ITP output.
		As previously mentioned, the validity of a goal in 
		the ITP corresponds to the unsatisfiability of its 
		negation in the ATP. If it is satisfiable, this 
		translates to a counterexample of the fact that 
		the goal holds. Hammers sometimes use this 
		feature of the ATP to avoid wasting effort 
		trying to prove unprovable sub-goals. More 
		interestingly, if the goal is found to be 
		unsatisfiable in the ATP, the refutation proof of 
		its unsatisfiability needs to be processed by the 
		ITP. In its simplest form, this pipeline consists 
		of using the ATP as an oracle, that is, to consider 
		the goal to be proven in the ITP if the ATP 
		concludes that its negation is unsatisfiable. 
		This compromises the ITP's trustworthiness and adds 
		to the the trust-base, the entire ATP.
		
		An improvement is to independently reconstruct 
		the proof in the proof assistant once the ATP finds 
		the goal to be unsatisfiable. In this case, the 
		only information the ITP gets from the ATP is that 
		the goal is provable given the premises. It does 
		not care about how the ATP proved this conjecture.
		Essentially, the ATP acts as a relevance filter for 
		the prover inside the ITP. This is done, for instance, 
		with Isabelle and the Metis 
		prover~\cite{10.1007/978-3-540-74591-4_18}. A more
		sophisticated but also more complex
		integration uses the proof steps used by 
		the ATP in addition to the premises
		to guide the construction of a proof in the ITP, 
		as done in 
		PRocH~\cite{10.1007/978-3-642-38574-2_18} which 
		reconstructs TPTP proofs in HOL Light.
		
\section{Sledgehammer}
\label{sec:sledgehammer}
	Sledgehammer is a component of Isabelle/HOL that uses 
	external ATPs to guide Isabelle/HOL's proof methods. 
	Sledgehammer uses both resolution provers and 
	SMT solvers for proof automation. When a user wishes 
	to find a proof for a conjecture using Sledgehammer, 
	the tool picks a few hundred relevant lemmas from 
	Isabelle's libraries and sends them along 
	with the conjecture to the ATP. The conjecture is 
	translated from Isabelle's polymorphic higher-order 
	logic (HOL) to the ATP's first order logic (FOL).
	Sledgehammer sends this query in parallel to
	all available ATPs. If an ATP is able to prove 
	the conjecture, Isabelle's own FOL subprover - Metis - 
	sets out to reprove the goal with the same premises. 
	Essentially, the role of the ATP in this integration 
	is to filter out relevant facts for Metis to 
	efficiently work with and also to eliminate 
	disprovable sub-goals from Metis's search space.
	This is useful in Isabelle because Metis is 
	considerably slower	than the external ATPs.
	
	In the rest of this section, we expand on the work 
	done to integrate SMT solvers with Isabelle/HOL via 
	Sledgehammer. Sledgehammer provides the \textit{smt}
	tactic to utilize this feature.
	
	This integration, like other hammer integrations, 
	consists of a premise selection phase, a translation 
	from Isabelle's HOL to the SMT solvers' FOL, and the 
	reconstruction of proofs found by the SMT solver with 
	inference rules of Isabelle/HOL. Reconstruction involves 
	either passing the minimal amount of facts needed 
	to prove the goal by the external solver to Metis, 
	Sledgehammer's internal prover or, Sledgehammer 
	can reconstruct the proof of the external prover, 
	inference by inference.	The SMT solvers integrated in 
	Sledgehammer are Z3, Yices~\cite{Dutertre:cav2014}, 
	and CVC3~\cite{10.5555/1770351.1770397}. The 
	integration with CVC3 was later extended to 
	one with CVC4. Yices and CVC3 
	are trusted as oracles.	Trusting SMT-solvers is 
	not a dependable technique for previously mentioned 
	reasons. The integration with Z3 involves proof 
	production by Z3 and reconstruction of these proofs 
	in Isabelle.
	
	\subsection{Translation}
		The integration of Sledgehammer with SMT solvers 
		consists of a translation from HOL 
		to many-sorted FOL. The target 
		of the translation is the SMTLIB standard for 
		SMT-solvers. The translation is sound - 
		validity of the translated problem implies 
		validity of the original problem - but not 
		complete - validity of the original problem 
		does not necessarily imply validity of the 
		translated problem. HOL is more expressive 
		than FOL. As such, all first-order terms 
		are representable in HOL. Translation of this 
		first-order subset of HOL is pretty straightforward. 
		More interestingly, the translation has to deal 
		with certain higher-order features that have 
		no obvious first-order counterpart. Some of 
		the incompleteness of the translation does 
		come from the inability to translate some 
		of these features.
		
		The type system of HOL consists 
		of type variables and compound types.
		A type variable is a schematic type that 
		can be instantiated to any particular type.
		For instance, $\alpha \to \beta$ is the type
		of all functions from some type $\alpha$ to
		some type $\beta$. This represents a 
		polymorphic type which is handled using 
		monomorphization --- the process of generating
		all instances of a set of schematic terms based 
		on a set of monomorphic terms until a fixed point
		is reached. This process can be nonterminating. 
		The translation performs a fixed number of 
		monomorphization steps since the terms that 
		actually contribute to proofs are 
		typically those that are generated by the 
		first few steps. Compound types 
		are types constructed from other types --- 
		$\kappa\ \tau_1 ... \tau_n$ is a type composed of 
		the types $\tau_1, ..., \tau_n$. After 
		recursively monomorphizing the types 
		$\tau_1, ..., \tau_n$, the translation 
		represents the compound type 
		$\kappa\ \tau_1 ... \tau_n$ as a first-order 
		type $\kappa^n$.
		
		An anonymous function from HOL is translated 
		to a named function and a quantified constraint
		is added to specify the function's behavior.
		This is called $\lambda-$lifting.
		Specifically, a term $t$ that contains a $\lambda-$abstraction
		(anonymous function) $\lambda x.u$ is translated 
		to a term $t$ with all occurrences of this 
		anonymous function substituted with $c$ and 
		the constraint $\forall x. c\ x = u$ is added 
		as an axiom.

		HOL also allows for partial applications of 
		functions, which are handled in the translation
		by using a constant $\texttt{app}$ that 
		represents explicit applications. If $c$ has 
		arity $m+1$ and is represented in the problem 
		with at least $m$ arguments, then 
		$c\ t_1 ... t_m$ is represented as is, 
		and this term can be applied to another 
		argument $t_{m+1}$ as 
		$\texttt{app}\ (c\ t_1 ... t_m)\ t_{m+1}$ along 
		with an axiomatization of what it means 
		to construct terms with $\texttt{app}$: 
		\begin{center}
			$\forall t_1, t_2.\ \texttt{app}\ t_1\ t_2 
			= t_1\  t_2$.
		\end{center}
		
		The translation is less general than the 
		translation used by Sledgehammer for 
		superposition provers, in that, since 
		(monomorphic) types are embedded in SMT solvers, 
		some of the types from Isabelle such as integers 
		and reals are mapped to their corresponding SMT 
		types. In addition, this work was extended with 
		support for bit-vectors or machine integers
		~\cite{10.1007/978-3-642-25379-9_15}. The 
		Isabelle/HOL counterpart to the SMT-LIB 
		theory of bit-vectors was developed by 
		Dawson~\cite{DAWSON200955}. All the SMT-LIB
		bit-vector operations have corresponding 
		definitions in the bit-vector libraries of 
		HOL4 and Isabelle/HOL.
		
		An evaluation on this translation showed that 
		SMT solvers improved and complemented the 
		proof automation achieved with resolution 
		provers alone on many trivial and non-trivial 
		problems.
		
	\subsection{The Z3 Proof System}
		Z3's proof system is a sequent calculus that uses 
		38 proof rules. Some of these rules are presented 
		in \Cref{fig:z3-rules} using sequents 
		$\Gamma \vdash \varphi$ where $\Gamma$ is a set 
		of formulas called the hypotheses
		and $\varphi$ is a formula called the proposition. 
		A rule consists of one or more sequents as 
		premises and a sequent as the conclusion. 
		Similar to resolution provers, a proof is 
		represented as a tree in theory, but implemented 
		as a directed acyclic graph (DAG) by Z3 to 
		due to node sharing. A proof tree consists of axioms 
		from the proof system as leaves, and application 
		of rules as nodes, and since proof trees 
		document unsatisfiability of the input 
		formulas, the root of the tree concludes the 
		empty clause $\bot$, which is represented by 
		the sequent $\Pi^\prime \vdash \bot$ where 
		$\Pi^\prime$ is the \textit{unsat core} --- 
		a subset of the assertions $\Pi$
		initially given to Z3 to be shown unsatisfiable.
		
		\begin{figure}[t]
			\begin{equation*}
				\infer[\texttt{true}]
					{\vdash \top}
					{}
				\ \ \ \ \ 
				\infer[\texttt{asserted}]
					{\{\varphi\} \vdash \varphi}
					{\varphi \in \Pi}
				\ \ \ \ \ 
				\infer[\texttt{mp}]
					{\Gamma_1 \cup \Gamma_2 \vdash \varphi_2}
					{\Gamma_1 \vdash \varphi_1 & 
					\Gamma_2 \vdash \varphi_1 \to \varphi_2}
			\end{equation*}
			\begin{equation*}
				\infer[\texttt{hypothesis}]
				{\{\varphi\} \vdash \varphi}
				{}
				\ \ \ \ \ 
				\infer[\texttt{lemma}]
					{\Gamma \cup \{l_1,...,l_n\} \vdash \bot}
					{\Gamma \setminus \{l_1,...,l_n\} 
						\vdash \neg l_1 \lor ... \lor \neg l_n}
			\end{equation*}
			\begin{equation*}
				\infer[\texttt{iff$_\top$}]
					{\Gamma \vdash \varphi \leftrightarrow \top}
					{\Gamma \vdash \varphi}
				\ \ \ 
				\infer[\texttt{iff$_\bot$}]
					{\Gamma \vdash \varphi \leftrightarrow \bot}
					{\Gamma \vdash \neg \varphi}
				\ \ \ 
				\infer[\texttt{iff$_\sim$}]
					{\Gamma \vdash \varphi_1 \sim \varphi_2}
					{\Gamma \vdash \varphi_1 \leftrightarrow \varphi_2}
				\ \ \ 
				\infer[\texttt{refl$_\simeq$}]
					{\vdash t \simeq t}
					{}
			\end{equation*}
			\begin{equation*}
			\infer[\texttt{elim$_\land$}]
				{\Gamma \vdash l_i}
				{\Gamma \vdash l_1 \land ... \land l_n}
			\ \ \ 
			\infer[\texttt{elim$_\neg\lor$}]
				{\Gamma \vdash \neg l_i}
				{\Gamma \vdash \neg (l_1 \lor ... \lor l_n)}
			\ \ \ 
			\infer[\texttt{symm$_\simeq$}]
				{\Gamma \vdash t_2 \simeq t_1}
				{\Gamma \vdash t_1 \simeq t_2}
			\end{equation*}
			\begin{equation*}
				\infer[\texttt{trans$_\simeq$}]
					{\Gamma_1 \cup \Gamma_2 \vdash t_1 \simeq t_3}
					{\Gamma_1 \vdash t_1 \simeq t_2 & 
				 	\Gamma_2 \vdash t_2 \simeq t_3}
			 	\ \ \ \ \ 
			 	\infer[\texttt{comm$_\simeq,\approx$}]
			 		{\Gamma \vdash (t_2 \approx t_1) \simeq (t_2^{\prime} \approx t_1^{\prime})}
			 		{\Gamma \vdash (t_1 \approx t_2) \simeq (t_1^{\prime} \approx t_2^{\prime})}
			\end{equation*}
			\caption{Simple Z3 proof rules.}
			\label{fig:z3-rules}
		\end{figure}
		A representative subset of the proof rules are 
		documented in~\cite{DBLP:phd/dnb/Bohme12} and 
		some of them are repeated here to illustrate 
		the workings of the Z3 proof system. In addition
		to the simple rules from \Cref{fig:z3-rules},
		resolution is a rule that's central to the 
		proof calculus. Resolution was introduced in 
		\Cref{smt} and a special case of it 
		called unit resolution is represented as follows in 
		the sequent calculus of Z3, where 
		$I = \{1,...,n\}$ and $J$ is a non-empty 
		subset of $I$ :\\
		\begin{equation*}
			\infer[\texttt{unit-resolution}]
				{\Gamma \cup \bigcup\limits_{j \in J}\Gamma_j \vdash \bigvee\limits_{i \in I\setminus J} l_i}
				{\Gamma \vdash \bigvee\limits_{i \in I}l_i & \langle \Gamma_j \vdash \neg l_j \rangle_{j \in J}}
		\end{equation*}
		
		Additionally, Z3 has a set of rules for converting 
		input formulas into equisatisfiable formulas in 
		conjunctive normal form (CNF) with polynomial 
		complexity, based on Tseitin's CNF conversion 
		method~\cite{Tseitin1983}. Tseitin's method 
		introduces auxiliary variables and constraints 
		asserting their equivalence with sub-formulas 
		that aren't in CNF, until the entire formula 
		is in CNF. Z3 runs this algorithm lazily and 
		optimizes it to avoid introducing too many 
		new variables. A formula is in negation normal 
		form (NNF) if negations are applied only to
		variables and the only Boolean operators 
		in it are conjunction and disjunction. 
		NNF is an intermediate step in the CNF 
		conversion process when quantifiers are 
		involved and Z3 has a set of rules for 
		translation of formulas to NNF in linear time.
		
		The main rule for reasoning about equality 
		is the \textit{congruence} rule:
		\begin{equation*}
			\infer[\texttt{cong}]
				{\bigcup\limits_{j\in J}\Gamma_j \vdash f(t_1, ..., t_n) \simeq f(t_1^{\prime},...,t_n^{\prime})}
				{\langle \Gamma_j \vdash t_j \simeq t_j^{\prime} \rangle_{j \in J} & 
				\langle t_i \equiv t_i^{\prime} \rangle_{i \in I\setminus J}}
		\end{equation*}
		Equality ($=$), equivalence ($\iff$), and 
		equisatisfiability ($\sim$) are congruence 
		relations, and $\simeq$ represents any one 
		of these relations. $\equiv$ represents syntactic 
		equivalence of terms. $I$ and $J$ are defined 
		as above for resolution.
		
		Z3 is also proof producing for quantified formulas.
		$Q$ represents either of the two quantifiers --- 
		the universal quantifier $\forall$ or the 
		existential quantifier $\exists$ --- and 
		$\overline{x}$ represents a list of variables 
		$x_1,...,x_n$. The following 
		proof rule allows introduction of quantifiers:
		\begin{equation*}
			\infer[\texttt{intro$_Q$}]
			{\Gamma \vdash (Q \overline{x}.\varphi_1 [ \overline{x}]) \sim (Q \overline{x}.\varphi_2 [ \overline{x}])}
			{\Gamma \vdash \varphi_1 [\overline{x}] \sim \varphi_2 [\overline{x}]}
		\end{equation*}
		The universal quantifier can be eliminated by 
		instantiation and an existential quantifiers 
		can be eliminated using skolemization.
		\begin{equation*}
		\infer[\texttt{inst$_\forall$}]
			{\neg(\forall\overline{x}.\varphi[\overline{x}])
			\lor \varphi[\overline{x} \mapsto \overline{t}]}
			{}
		\end{equation*}
		Note the representation of implication $a \to b$ in 
		its CNF form $\neg a \lor b$.
		
		Skolemization~\cite{NONNENGART2001335} is the process 
		of removing existential quantifiers from formulas using 
		constants called Skolem constants as witnesses.
		\begin{equation*}
		\infer[\texttt{sk$_\exists$}]
			{(\exists\overline{x}.\varphi[\overline{y},\overline{x}]) 
			\sim \varphi[\overline{y},\overline{f}(\overline{y})]}
			{}\ \ \ \ 
		\infer[\texttt{sk$_\forall$}]
		{(\neg\forall\overline{x}.\varphi[\overline{y},\overline{x}]) 
			\sim (\neg \varphi[\overline{y},\overline{f}(\overline{y})])}
		{}	
		\end{equation*}
		
		Besides these, Z3 provides theory rules, 
		that conclude disjunctions of theory literals 
		called theory lemmas, for theory specific reasoning
		and rewriting rules. These are called 
		\texttt{th-lemma} rules. The theory of bit-vectors
		has special \texttt{th-lemma-bv} rules owing to 
		the fact that bit-vector proofs involve reasoning 
		about the corresponding bits of a bit-vector --- 
		a process called bit-blasting. 
		
		Rewriting is an important part of SMT solvers --- 
		this process takes 
		care of various simplifications and 
		transformations to canonical forms. Z3 
		provides a set of metatheorems that are 
		rewriting patterns that may repeatedly 
		occur such as symmetry of equality, common 
		propositional tautologies, etc. Sledgehammer
		allows users to add rules to the set of 
		metatheorems. If one of Z3's rewrite rules 
		fails to be reconstructed in Sledgehammer, 
		the user can add a metatheorem that 
		matches the pattern to fix the failure.
		For the bit-vector 
		extension~\cite{10.1007/978-3-642-25379-9_15}, 
		schematic theorems that play the same roles 
		as metatheorems have proved to be crucially 
		time-saving. If a schematic theorem's conclusion 
		matches a term, the entire theorem is 
		instantiated by the scheme. Besides commutativity 
		and associativity rules specific to bit-vector
		operations, examples include simplification 
		rules such as the neutrality of the bit-vector 
		representing $0$ for bit-wise disjunction.
		
	\subsection{Proof Reconstruction}
		Sledgehammer \textit{smt} tactic trusts the Yices 
		and CVC3 solvers, so only proofs produced 
		by Z3 are reconstructed in Isabelle.
		The Z3 proofs are reconstructed step-by-step,
		and currently supported theories are equality, 
		linear arithmetic, and bit-vector arithmetic.
		Since Z3's FOL is a subset of Isabelle's HOL,
		formulas in Z3 proofs are easily representable 
		as terms in Isabelle/HOL. The Z3 proof tree
		is scanned in depth-first post-order, starting 
		from the root node, and reconstructed in Isabelle,
		step-by-step. Each proof node, contains information 
		such as rule name, references to premises and 
		propositions, and is represented as an 
		algebraic datatype with a unique ID assigned
		by Z3. Sledgehammer stores these in a balanced 
		tree with logarithmic lookup using the IDs as
		keys. Reconstruction of a proof node 
		represents a proven Isabelle/HOL theorem of 
		the node's conclusion. Every node's 
		premises are discharged as theorems, which are then 
		used to derive the node's conclusion. Once 
		reconstructed, a proof node can be reused 
		by Sledgehammer. If $\Pi$ is the set of asserted 
		formulas, $\Pi^\prime$ represents the unsat 
		core returned by Z3 --- the minimal subset of 
		$\Pi$ required to prove the problem unsat. 
		After reconstructing the root node, Sledgehammer 
		checks the rest of the tree to see that only the 
		formulas from $\Pi^\prime$ remain as leaves.
		
		Proof reconstruction is sound by construction 
		and does not extend the trusted code base
		beyond Isabelle's LCF kernel, but it is 
		incomplete. In fact, this work led to the 
		discoveries of soundness bugs in Z3 that were subsequently fixed by the developers.
		
		For the theory of bit-vectors, the \textit{smt}
		tactic involves some additional optimizations 
		to offset the complexity added by bit-blasting - 
		a process that can cause an exponential blow-up. 
		One of these - schematic theorem instantiation - 
		is described in the previous sub-section. 
		Memoization of theorems, that is, caching of 
		certain computationally expensive bit-vector 
		specific theorems in a structure called a term net 
		also proved useful. These techniques, and many 
		useful bit-vector rewrite rules --- are used 
		with HOL4 and Isabelle/HOL's existing automation
		for bit-vector reasoning.
		
		Since Z3's proofs are relatively coarse-grained,
		proof reconstruction in Sledgehammer has to perform
		proof search when the proof step is not reflected 
		in Z3's proof. According to 
		~\cite{DBLP:phd/dnb/Bohme12}, enhancing Z3 with 
		more descriptive rules would 
		help improve the \textit{smt} tactic and strengthen 
		the integration.
		
\section{SMTCoq}
\label{sec:cert}
	SMTCoq implements a skeptical cooperation between 
	Coq and proof-producing SMT  and SMT solvers. 
	It can be used as a proof checker for proofs 
	produced by SAT and SMT solvers. More relevantly, 
	it is a Coq plugin that uses SAT and SMT solvers to 
	prove goals in Coq. It is written and proved 
	correct in Coq, with some OCaml support for 
	translation and preprocessing. SMTCoq allows 
	the user to query a number of SAT/SMT solvers 
	and if the solver is able to validate the query, 
	that is, it determines that the negation of the 
	conjecture is unsatisfiable, it produces a proof 
	of unsatisfiability, which is checked in Coq.
	
	SMTCoq's uses \textit{computational reflection}~\cite{113737} 
	to invoke Coq's computation within 
	proofs. Proofs can be shortened by increasing 
	their computational	part, due to the 
	Curry-Howard isomorphism that Coq is based on. 
	Reflection requires the SMT solver's first-order
	terms to have two representations. FOL terms are 
	represented in Coq using Coq's first-order 
	terms in a \textit{shallow embedding} and also 
	using datatypes defined in Coq --- the 
	\textit{deep embedding}. Thus, FOL statements 
	are naturally represented in Coq in its 
	shallow embedding. Reflection allows to 
	prove these statements using computations 
	on their deep embedding. SMTCoq provides 
	the tools necessary to switch between the 
	two embeddings --- an \textit{interpretion} 
	function compiles deep terms to their Coq 
	counterparts; and \textit{reification} 
	- written in OCaml without compromising 
	soundness --- transforms shallow terms to the 
	corresponding deep terms.
	
	SMTCoq represents the quantifier free formulas 
	from the SMT solver in Coq as Boolean terms. Thus, 
	a Boolean decision procedure in SMTCoq, checks 
	proof certificates from solvers. However, 
	\texttt{Prop} is the type of propositions 
	in Coq, and thus is the type of proofs. 
	The Ssreflect plugin allows reflection from 
	the \texttt{bool} type in Coq to \texttt{Prop}.
	This is crucial since Booleans are easier to 
	manipulate via computations and propositions are 
	harder to prove.
	
	SMTCoq is sound --- proven to be correct in Coq 
	- but not complete. This means that when an 
	SMT solver proves a conjecture, we can we be 
	confident of its response since we get a 
	\texttt{Qed} proof in Coq. However, if 
	SMTCoq is not able to prove a conjecture, 
	we cannot be sure that its unprovable.
	Since certain theories can make SMT 
	solving undecidable, completeness is not a 
	realistic goal.	
	
	Since different SMT solvers produce resolution 
	proofs of unsatisfiability in different formats, 
	SMTCoq has its own input certificate format. 
	The proofs from the different SMT solvers are 
	translated to an SMTCoq certificate which 
	can then be checked within Coq. Given an input 
	query, the SMT solver first converts it into CNF
	(conjunction normal form) which is better 
	suited for resolution proofs. The final 
	resolution proof might also consist of smaller 
	proofs of theory lemmas from the theory solver. 
	
	\begin{figure}[t]
		\centering
		\includegraphics[scale=0.4]{tactic_cex.pdf}
		\caption{SMTCoq's integration with SMT solvers. Credits: Alain Mebsout}
		\label{fig:smtcoq}
	\end{figure}
	
	SMTCoq takes a modular approach of checking an
	SMT proof. The checker is divided into a main checker 
	that delegates parts of the proof to small checkers. 
	The proof is divided into steps that small checkers 
	can check. There is a small checker for CNF 
	conversion, one for resolution, and one for each theory. 
	SMT solvers perform a preprocessing 
	step to rewrite certain input formulas to 
	a simplified version, and SMTCoq has a step 
	and a checker corresponding 
	to this rewriting phase. The main checker divides the 
	proof into steps, gives the step to the relevant 
	small checker, and checks that in the 
	end, the empty clause is deduced from the initial query.
	Since this initial query is the negation of our 
	conjecture, deriving the empty clause from it 
	signifies its unsatisfiability. Each small checker 
	operates independently and maintains an invariant 
	that allows the checking process to be split this way.
	Specifically, the correctness of the main checker 
	depends on the correctness of each small checker.
	Since each small checker transforms the initial 
	query, generated from the initial goal,
	by replacing clauses by new ones, its 
	correctness guarantees that it will not make
	unsound transformations. In other words, the 
	small checker guarantees that its transformations 
	preserve the (un)satisfiability of the current query.
	
	SMTCoq uses Coq's native arrays to store the clauses, 
	a set of which represent the goal. The main checker 
	handles this initial array of clauses representing 
	the negation of the goal in CNF, and each small 
	checker computes a clause that is implied from a 
	subset of the clauses. For efficiency, SMTCoq 
	replaces the unnecessary clauses with new ones 
	when it knows that they will not be useful anymore. 
	After all the steps are handled by the small 
	checkers, the main checker checks that the final 
	implied clause is the empty clause. Sub-terms of 
	deep terms are hash-consed (i.e., cached for maximal 
	term sharing. Additionally, variables and literals 
	of FOL are represented in Coq using machine 
	integers on which fast computations can be 
	performed to manipulate them.
	
	Currently SMTCoq supports the SAT-solvers 
	zChaff~\cite{10.1007/11527695_27} and 
	Glucose~\cite{10.5555/1661445.1661509}, and 
	the SMT-solvers CVC4 and veriT. 
	
\section{Comparison}
\label{sec:comp}
	Sledgehammer has three possible integrations with 
	SMT-solvers: it either 
	$(i)$ trusts the SMT-solver as an oracle, 
	$(ii)$ uses the SMT-solver as a relevance filter 
	on premises so it can prove goals efficiently 
	using Metis, or 
	$(iii)$ reconstructs proofs produced by the 
	SMT solver to produce goals without compromising 
	the soundness guarantees of the small LCF-kernel. 
	The third of these is most comparable to how 
	SMTCoq operates --- it sends Coq conjectures to 
	the SAT or SMT solver, which returns a proof of 
	the conjecture if it finds it to be provable; 
	SMTCoq then checks this proof within Coq by 
	reflection. SMTCoq can also be used as a checker 
	for proofs produced by SMT solvers. In fact, 
	while there exists an independent LFSC checker 
	for LFSC proofs provided by CVC4, veriT produces 
	proofs but does not have a dedicated checker. It 
	relies on integrations with proof assistants to 
	check its output. In this section, we compare the 
	usage of SMTCoq and Sledgehammer according to 
	their ability to use SMT-solvers to automate 
	proofs within their respective ITPs.
	
	Recall that ITPs have more expressive logics 
	than ATPs, and that roughly, the logic of an ATP 
	is a subset of that of an ITP. So they can be used 
	to automate goals that are expressed within this 
	subset. Sledgehammer goes a little further by 
	describing a translation of certain HOL 
	features into FOL features understood by SMT 
	solvers. While this translation is incomplete, 
	it captures enough HOL features to widen the set 
	of problems that an SMT solver can help automate. 
	Since SMTCoq does not deal with Coq's non-FOL 
	constructs, SMTCoq's tactics cannot be called 
	on Coq goals that contain, for instance, anonymous 
	functions, partial function applications, or 
	higher-order terms in general. SMTCoq does not deal 
	with polymorphic types or compound types either. 
	There is a correspondence between types in Coq 
	and theories in SMT, and only Coq goals that 
	have these particular types can use the SMT solver. 
	The integer sort from SMT-LIB is mapped to Coq's 
	$Z$ type, and Coq's $Bool$ type 
	corresponds to propositional reasoning in SAT or SMT.
	However, SMTCoq defines custom types 
	for machine integers and arrays. While these serve as 
	rich, independent Coq libraries for these types, 
	they are not standardized in Coq. In contrast,
	Sledgehammer only supports the theories of linear 
	integer arithmetic and bit-vectors, besides equality 
	over uninterpreted functions which is also supported 
	by SMTCoq. Additionally, SMTCoq does not have support 
	for quantified formulas. Although the goal in Coq is universally quantified, on the SMT 
	side, it amounts to checking the unsatisfiability of a quantifier-free formula. This is not the case with 
	formulas containing either alternating quantifiers, or 
	those that occur outside the head of the formula --- 
	SMTCoq will fail on these. Sledgehammer uses Z3's 
	quantifier proofs to support quantified fragments of 
	its theories.
	
	Premise selection is a crucial part of Sledgehammer. A lot 
	of research has gone into optimizing premise selection to 
	make proving more efficient. Consequently, the SMT 
	integration also uses Sledgehammer's premise selection 
	mechanisms to supply the SMT solver with the best 
	possible hypotheses to prove a goal. This process is 
	so imperative that using the SMT solver just as a 
	relevance filter for premises has given useful 
	results. In SMTCoq, each lemma is considered an 
	independent entity with its set of hypotheses $H$ and 
	goal $G$ that are translated to an SMT-LIB problem. 
	If the proof of $G$ depends on conjectures stated
	outside of $H$, the SMT solver will likely not be 
	able to prove them. This difference in 
	ideology comes from the difference in axiomatizations
	of theories in SMT solvers and resolution provers. 
	Sledgehammer was originally coupled only 
	with resolution provers that do not have a notion of 
	theories. Any additional axioms of a theory 
	must be appended to the input of a superposition 
	prover. SMT solvers have theories encoded into them, 
	which might have facts besides $H$ that might help 
	prove $G$. However, this might not always be the case.
	In the next section, we propose an extension to SMTCoq 
	using abduction to deal with this shortcoming.
	
	The Sledgehammer integration with SMT solvers is 
	specific to Z3 (although there are looser integrations 
	with other SMT solvers). Although SMT solvers have 
	a common input/output syntax in SMT-LIB, their 
	proof formats are quite different. Extending 
	Sledgehammer's \textit{smt} tactic to other 
	SMT solvers might not be straightforward due to 
	the idiosyncrasies of Z3. On the other hand, SMTCoq
	claims to be generic and extensible. SMTCoq has a 
	certificate format, that is different from the proof 
	formats of each SMT solver. To extend SMTCoq's 
	support with a new solver, one only needs to 
	write a preprocessor that translates proofs from 
	this solver's format to that of the SMTCoq certificate.
	The SMTCoq checker's soundness lemmas are also 
	general enough to make it possible to add more 
	theories simply by extending the type of terms 
	in SMTCoq. These claims are corroborated
	by several successful extensions to SMTCoq. Initially, 
	it had support only for the ZChaff SAT solver and 
	the veriT SMT-solver, with the ability to reason 
	in the theories of EUF and LIA. Since its inception, 
	the CVC4 SMT solver and the theories of bit-vectors 
	and arrays have been added to SMTCoq's
	capabilities~\cite{DBLP:journals/corr/EkiciKKMRT16}.

\section{Related Work}
\label{sec:rel}
	In this section, we present other integrations 
	between ATPs and ITPs, and related research,  
	that we didn't get into deeply in this work. \cite{10.1007/10721959_10}
	presents the framework for embedding 
	the resolution calculus within type theoretic 
	ITPs. Some instances of this embedding 
	are: in~\cite{Ahrendt1998}, between the 
	$_{3}TAP$~\cite{10.5555/648232.753129} ATP 
	and the Karlsruhe Interactive Verifier 
	(KIV)~\cite{Reif1995}; 
	IVY~\cite{10.1007/10721959_30}, a verified 
	prover in the ACL2 
	framework~\cite{10.1007/978-3-540-71067-7_4} 
	that calls the 
	Otter~\cite{10.1023/A:1005843632307} theorem 
	prover. Connections between ITPs and SMT 
	solvers include ones between the PVS 
	system~\cite{10.1007/3-540-55602-8_217}
	and the Yices SMT solver
	in~\cite{1698746}, between the UCLID
	solver~\cite{10.1007/978-3-540-27813-9_40}
	and ACL2 in~\cite{DBLP:conf/iccad/ManoliosS05}, 
	and haRVey (now veriT) with Isabelle/HOL 
	in~\cite{10.1007/11691372_11}.

	CoqHammer~\cite{article} is a hammer for the 
	Coq proof assistant. It's novelty comes from 
	the fact that most current hammers are for 
	LCF-style provers that have a HOL that is 
	different from CIC. CoqHammer has a translation
	from Coq's CIC to untyped FOL which has a 
	practical level of soundness and completeness.


\section{Conclusion and Future Work}
\label{sec:conc}
	We have looked at various integrations between 
	automatic and interactive theorem provers.
	Hammers that typically utilize resolution 
	provers within ITPs have complex premise 
	selection methods to pick an optimal set 
	of lemmas that will allow the ATP to prove 
	the goal. We looked at an extension of 
	Sledgehammer that applies this framework 
	with an SMT solver. On the other hand, 
	SMTCoq queries SAT and SMT solvers with 
	independent lemmas in Coq and certifies the 
	solver's result within Coq using computational
	reflection. 
	
	These illustrate the currently popular solutions 
	to the premise selection problem - use smart 
	methods to find a good set of premises that will 
	enable the ATP to find a proof; alternatively, 
	hope that the ATP's	axiomatization is strong enough 
	to have any	extra information necessary, or fail 
	otherwise. We suggest involving the user in the 
	premise selection process with some help from the 
	ATP. Our proposal involves using CVC4's abduction
	solver~\cite{DBLP:conf/cade/ReynoldsBLT20} in 
	SMTCoq to do this. For a theory $T$, given a
	set of axioms $A$ and a goal $G$, abduction 
	finds a formula $\phi$ (the abduct) --- if it 
	exists --- such that 
	\begin{center}
		$A \land \phi \models_{T} G$
	\end{center}
	In other words, it finds a formula $\phi$ that 
	is consistent with the axioms and when added 
	to them, allows for the goal to be proven. 
	When SMTCoq sends CVC4 a lemma $H \models G$
	and CVC4 finds that $G$ does not follow from $H$, 
	the idea would be to have it 
	return a set of abducts for $G$ and $H$. The user 
	would then prove the relevant abduct from 
	previously proven lemmas and query CVC4 
	with that abduct added to $H$, thus enabling 
	CVC4 to prove the goal. We hope to take SMTCoq's 
	CVC4 integration forward this way and avoid the 
	premise selection problem.
\bibliographystyle{abbrv}
\bibliography{bib}

\end{document}